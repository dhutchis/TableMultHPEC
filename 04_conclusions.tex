%!TEX root =SpGEMM_Accumulo_HPEC.tex

\section{Discussion}
\label{sDiscussion}

\subsection{Related Work} %Analogy to MapReduce with Accumulo Scanners, Iterators and BatchWriters:
%\todo[inline]{Cannon's algorithm, other SpGEMM}
Bulu\c{c} and Gilbert studied message passing algorithms for SpGEMM
such as Sparse SUMMA, most of which use 2D block decompositions \cite{buluc2012parallel}.
Unfortunately, 2D decompositions are difficult in Accumulo 
and message passing even more so.
In this work we use Accumulo's native 1D decomposition along rows 
and no tablet server communication
other than shuffling partial products to tablets of $\matr{C}$ via BatchWriters.


Our outer product method could have been implemented in MapReduce %x\cite{dean2008mapreduce} 
on Hadoop or its successor YARN \cite{vavilapalli2013apache}.
In fact, there is a natural analogy from TableMult to MapReduce:
the map phase scans rows from $\matr{A^\tr}$ and $\matr{B}$
and generates a list of partial products from TwoTableIterator;
the shuffle phase sends partial products to the correct tablets of $\matr{C}$ via BatchWriters;
the reduce phase sums partial products using Combiners.
Examining the conditions on which MapReduce outperforms Accumulo-only solutions
is worthy future work.

%TableMult Design Alternatives
A common Accumulo pattern is to %run multiple clients %that scan disjoint and continuous table sections in parallel.
scan and write from multiple clients in parallel.
In fact, the current insert rate record was set by using 
parallel clients and tablet location info \cite{kepner2014achieving}.
%% Kepner et al used parallel clients and tablet location info
%% to insert at record-setting rates \cite{kepner2014achieving}.

We avoid the multiple client pattern because it increases client software complexity,
whereas we aim for a service within Accumulo that works for any client.
Perhaps more importantly, previous work has shown that table scans 
that do not perform significant iterator processing %filtering or other server-side computation 
bottleneck on communication overhead at the client related to Apache Thrift serialization \cite{sawyer2013understanding}.
We gain a chance to eliminate this overhead by moving computation to the server,
though we do not currently do so as we use standard Scanners and BatchWriters.


\subsection{Design Alternative: Inner-Outer Product Hybrid}

%bridge inner and outer product
It is worth reconsidering the inner product method from our initial design
because it has an opposite performance profile as 
Figure~\ref{fInnerOuterSpectrum}'s left and right depict: 
inner product bottlenecks on scanning whereas outer product bottlenecks on writing.
At the expense of multiple passes over input matrices, inner product emits 
partial products in order and immediately pre-summable,
%more efficiently in that emitted entries are in order and partial products can sum immediately, 
reducing the number of entries written to Accumulo to the minimum possible.
Outer product reads inputs in a single pass
but emits entries out of order and has little chance to pre-sum, 
instead writing individual partial products to $\matr{C}$.
Table~\ref{tResultsParams} quantifies that outer product writes
2.5 to 3 times that of inner product for power law inputs.
In the worst case, multiplying a fully dense $n \times m$ with an $m \times p$ matrix,
outer product emits $m$ times more entries than inner product.



%LRU cache enables the bridge
Is it possible to blend inner and outer product SpGEMM methods,
choosing a middle point in Figure~\ref{fInnerOuterSpectrum}
with equal read and write bottlenecks for overall greater performance?
In the following generalization, 
free parameter $P$ varies behavior between
inner product at $P=n$ and outer product at $P=1$:

\removelatexerror
\begin{algorithm}[H]
\vspace{\algspace}
\SetKwBlock{fore}{for}{} 
\SetKw{emit}{emit}
\fore($l = 1\col P$){
\fore($k = 1\col m$){
\fore({$i = \left( \floor*{\dfrac{(l-1)n}{P}}+1 \right) \col \floor*{\dfrac{ln}{P}}$}){
\fore($j = 1\col p$){
%\vspace{4pt}
\emit{$ \matr{A}(i,k) \otimes \matr{B}(k,j)$}
}}}}
\vspace{\algspace}
\end{algorithm}

The hybrid algorithm runs $P$ passes through $\matr{B}$,
each of which has write locality to a vertical partition of $\matr{C}$ of size $\ceil{n / P}$.
Pre-summing ability likewise varies inversely with $P$, 
though actual pre-summing depends on
$\matr{A}$ and $\matr{B}$'s  sparsity distribution
as well as how many positions of $\matr{C}$ the TableMult iterators cache.
Figure~\ref{fInnerOuterSpectrum}'s center depicts the $P=2$ case.

\begin{figure}[htb]
%\vspace{-6pt}
\centering
\includegraphics[width=\linewidth]{InnerOuterSpectrum}
\vspace{-12pt}
\caption{Tradeoffs between Inner and Outer Product}
\label{fInnerOuterSpectrum}
\vspace{-6pt}
\end{figure}


A challenge for any hybrid algorithm is mapping it to Accumulo infrastructure.
We chose outer product because it more naturally fits Accumulo, 
using iterators for one-pass streaming computation, 
BatchWriters to handle unsorted entry emission and Combiners to defer summation.
The above hybrid algorithm resembles 2D block decompositions,
and so maximizing its performance may be challenging 
given limited data layout control and unknown data distribution.
%Further investigation is future work.
%Hybrid solutions might consider locality groups or transpose tables to enable column-oriented scanning
%and the distribution of tablets to tablet server cost modeling to 
%% We may realize greater performance by considering data placement among tablet servers, 
%% but such a consideration would require accessing and perhaps manipulating
%% Accumulo's internal state and non-public API.
%% We suggest this paper's approach as a balance between top performance and implementation stability.


\subsection{TableMult in Algorithms}
Several optimization opportunities exist for TableMult as a primitive in larger algorithms.
Suppose we have a program $\matr{E} = \matr{AB}; \matr{F} = \matr{CD}; \matr{G} = \matr{EF}$.
We would save two round trips to disk if we could mark $\matr{E}$ and $\matr{F}$ as 
``temporary tables,'' i.e. intermediate tables to an algorithm that should be held in memory 
and not written to Hadoop if possible.

A \emph{pipelining} optimization streams entries from a TableMult 
to computations taking its result as input. 
Outer product pipelining is difficult
because it cannot guarantee all partial products for any particular element 
are written to table $\matr{C}$ until it finishes.
Inner product's write locality makes it easier to pipeline.
More ambitiously, a \emph{loop fusion} optimization merges iterator stacks 
for two computations into one. 

Optimizing computation on NoSQL databases is challenging in the general case because
NoSQL databases mostly exclude query planner features 
customary of SQL databases in exchange for raw performance.
NewSQL databases aim in part to achieve the best of both worlds---performance and query planning \cite{grolinger2013data}.
We aspire to make a small step for Accumulo in the direction of NewSQL with current Graphulo research.






\section{Conclusions}
\label{sConclusions}

In this work we showcase the design of TableMult, a Graphulo implementation of the 
SpGEMM GraphBLAS linear algebra kernal server-side on Accumulo tables.
We compare inner and outer approaches and show how outer product 
better fits Accumulo's iterator model.
Performance experiments show good weak scaling and hint at strong scaling,
although repeating experiments on a larger cluster is necessary to confirm.

Current research is to implement the remaining GraphBLAS kernels 
and develop algorithms calling them, % atop the Graphulo library,
ultimately delivering a Graphulo linear algebra library 
as a pattern for server-side computation
to the Accumulo community.
