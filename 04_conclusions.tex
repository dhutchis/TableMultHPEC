%!TEX root =SpGEMM_Accumulo_HPEC.tex

\section{Discussion}
\label{sDiscussion}

Our initial design operated the iterator stack on a full major compaction.
We chose to operate the iterator stack on a scan instead because major compactions experience
a delay on the order of seconds before Accumulo schedules them, slightly bumping latency,
and because opening a BatchWriter inside an iterator at major compaction presents a small chance for deadlock.
Deadlock may occur if the major compaction iterators triggered enough minor compactions 
such that they exhaust every thread in the compaction thread pool.
This leaves open the chance that a major compaction thread would block on a minor compaction thread
in order for the BatchWriter to write entries, while in turn the minor compaction thread blocks on 
the major compaction since major compactions take a higher thread pool priority than minor ones.

%\todo[inline]{temporary tables}
One frequent computaional pattern is to create intermediary tables in the middle of an algorithm 
that are not needed once the algorithm completes.  For example, we may have a series of TableMult operations 
for which only the final TableMult is needed as output.  It is therefore wasteful to write the intermediary
TableMult results to disk if we have room to store them in memory.
Sometimes one can restructure an algorithm to minimize the use of intermediary tables,
but a better solution would be to realize a notion of in-memory ``temporary tables'' in Accumulo.
We leave constructing this notion to future work.

Similarly, it is also useful to \emph{pipeline} TableMults in an algorithm by starting the process 
that acts on the result of a TableMult before the TableMult finishes.
Unfortunately, the outer product algorithm cannot guarantee that all partial products for a particular element 
are written to the result table before the algorithm finishes, since it writes results in chaotic order.
The inner product algorithm would be easier to pipeline.
We therefore treat TableMult operations as barriers for operations acting on a TableMult's result.


We implemented TableMult logic inside three separate iterators connected via the SortedKeyValueIterator
interface. One technique for increasing performance is \emph{loop fusion}, combining separate components into one
that performs everything in one pass.  We chose to keep our iterators separate because it opens them 
to reuse in other server-side operations and the iterator's processing (including necessary decoding and encoding)
is not our bottleneck. We will be bound by BatchWrite time no matter how well we optimize the iterator processing.


\section{Related Work} %Analogy to MapReduce with Accumulo Scanners, Iterators and BatchWriters:
Our outer product method could also have been implemented in MapReduce \cite{dean2008mapreduce} 
on Hadoop or its successor YARN \cite{vavilapalli2013apache}.
In fact, there is a natural analogy from
how we process data using Accumulo infrastructure to methods in MapReduce.

In the map phase, we map matching rows of the input tables to a list
of partial products generated from their outer product.
We realize the map phase in Accumulo via the Scanners and the TwoTableIterator.
In the shuffle phase, we send partial products to the correct tablet of the result
table via machinery in the Accumulo BatchWriter (using data in the metadata table).
In the reduce phase, partial products are lazily combined by Accumulo combiners 
that implement the $\oplus$ operation.

We have a hunch that a MapReduce implementation using the AccumuloInputFormat and AccumuloOutputFormat 
will outperform our implementation in terms of throughput for large input tables (i.e. whole-table analytics)
but not latency for moderate input tables (i.e. queued analytics), due to spinup delay MapReduce jobs experience.
In either case, using MapReduce requires features in Hadoop clusters outside the 
Accumulo ecosystem, which may not be an option for some user environments.
It would be interesting to directly compare a MapReduce implementation in the future.

\todo[inline]{Cannon's algorithm, other SpGEMM}

\todo[inline]{Stored Procedures a la MySQL}

% Talk about how we measured performance?
To measure performance, we used techniques similar to those from Google Dapper \cite{sigelman2010dapper}.
We instrument sections of code inside try-finally statements, recording the start time before 
entering those sections and ensuring we record stop times inside the finally portion.
We store total time spent inside these code section ``spans'' inside thread-local storage,
along with statistics on how many times we enter a span and the minimum and maximum amount of time
spent inside a span. These statistics showed us what code was being traversed more often than expected
and what portions of code took the longest. 

\section{Conclusions}
\label{sConclusions}

In this work we showcase the design of TableMult, a Graphulo implementation of the 
SpGEMM GraphBLAS linear algebra kernal server-side on Accumulo tables.
We compare inner and outer approaches for SpGEMM and show how outer product is better
suited to the Accumulo iterator environment.
Performance experiments show ideal weak scaling and hint at good strong scaling,
although repeating our experiments on a larger cluster is necessary to confirm.

Current research for Graphulo is to implement the rest of the GraphBLAS kernels 
and develop algorithms calling them, % atop the Graphulo library,
ultimately presenting a Graphulo linear algebra library 
and demonstrating a pattern for server-side computation pattern
to the Accumulo community.
