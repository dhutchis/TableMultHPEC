%!TEX root =SpGEMM_ACCUMULO_HPEC.tex

\section{Introduction}
\label{sIntro}
% 

%Accumulo is well-known as a high performance NoSQL database with respect to ingest and scans \cite{sen2013benchmarking}.
%The next step past ingesting and scanning is computing---running enrichment, algorithms and analytics.

NoSQL databases such as Apache Accumulo
concentrate on high performance ingest and scans~\cite{sen2013benchmarking}. 
While fast ingest and scans solve some big data problems,
more complex scenarios involve running tasks
such as data enrichment, graph algorithms, and clustering analytics. These techniques
often move data from a database %or storage engine 
to a compute node. The ability to
%perform enrichment, algorithms or analytics directly in a database or storage engine
compute directly in a database can lead to benefits including 
%can provide benefits such as 
\emph{selective access}, \emph{data locality}, and \emph{infrastructure reuse}. 

Apache Accumulo is designed to deliver
fast answers to queries for data subsets along indexed attributes. 
% Conducting computation within a database like Accumulo offers several advantages,
% assuming we can efficiently map it to the database's programming model:
% %as opposed to YARN or MapReduce directly on HDFS, are 
The Accumulo server processes typically run on the physical nodes where data is stored and cached.
Performing computations inside Accumulo can avoid unnecessary network transfer,
effectively moving ``compute to data''
in contrast to client-server models that move ``data to compute.''
Performing computations in the Accumulo server also reuses its distributed infrastructure
such as write-ahead logging, fault-tolerant execution, and 
parallel load balancing of data.

%cite linear algebra on database data?
There are variety of ways to store graphs in a Accumulo.  One common schema is
to store the graph as a sparse matrix.  Graph algorithms performed on graphs
represented as sparse matrices can be easily implemented with a small number
of matrix operations. 
Researchers in the GraphBLAS Forum \cite{mattson2014standards} 
have identified a set of kernels
that form a basis for matrix algorithms useful for graphs.
This article presents Graphulo, an effort to realize the GraphBLAS primitives 
to enable server-side algorithms using matrix mathematics in Accumulo \cite{gadepally2015gabb}.

%I don't want to make something like MapReduce-- Accumulo facilitates a particular kind of computation 
%using iterators.  Not all computation patterns fit into the iterator framework. EXAMPLE
%We shouldn't stuff computation contradictory to the iterator framework into Accumulo, 
%as many have done stretching iterative algorithms into the MapReduce framework.

In this paper we focus on Sparse Generalized Matrix Multiply (SpGEMM), the core kernel at the heart of the GraphBLAS.
Many GraphBLAS primitives can be expressed in terms of
SpGEMM via the GraphBLAS user-defined element-wise multiplication and addition functions. 
SpGEMM can be used to implement a wide range of algorithms from graph search \cite{kepner2011graph} to table joins \cite{cohen2009mad} and many others (see \cite{bulucc2010highly}).

We call our implementation of SpGEMM on Accumulo \textsc{TableMult}, short for multiplication of Accumulo tables.
Accumulo tables have many similarities to sparse matrices, though a more precise mathematical definition is Associative Arrays 
\cite{kepner2014gabb}. For this work, we concentrate on
large distributed tables that may not fit in memory and use a streaming
approach that leverages Accumulo's builtin distributed infrastructure.
% Because Accumulo tables may span Hadoop clusters,
% we never assume a table fits in memory
% and instead take a streaming approach that distributes with Accumulo's infrastructure.

We are particularly interested in Graphulo for queued analytics, that is, analytics on selected table subsets.  
Queued analytics maximally leverage  databases
by quickly accessing subsets of interest, 
whereas whole-table analytics may perform better on parallel file systems such as Lustre or Hadoop.
We therefore prioritize smaller problems that require low latency
to enable analysts to explore graph data interactively.

%\subsection{Paper Outline}

We review Accumulo and its model for server-side computation, iterator stacks, 
in Section~\ref{sAccumuloIterators}.
We formally define matrix multiplication and compare inner and outer product SpGEMM methods
in Section~\ref{sMatMul}, ultimately settling on outer product for implementing TableMult.
We show TableMult's design as Accumulo iterators in Section~\ref{sTableMult}
and test its scalability with experiments in Section~\ref{sPerformance}.
We discuss design alternatives and related work in Section~\ref{sDiscussion}, 
concluding in Section~\ref{sConclusions}.

%Background and Algorithms
%Primer


%\section{Background}
%\label{sBackground}




\subsection{Primer: Accumulo and its Iterator Stack}
\label{sAccumuloIterators}
%key, value, entry, tablet
Accumulo stores data in Hadoop RFiles as byte arrays indexed by key using (key, value) pairs called entries.
Keys decompose further into 5-tuples consisting of a row, column family, column qualifier, visibility and timestamp.
For simplicity, we focus on a 2-tuple key consisting of a row and column qualifier.
Entries belong to tables, which Accumulo divides into tablets and assigns to tablet servers.
Client applications write new entries via BatchWriters 
and retrieve entries sequentially via Scanners
or in parallel via BatchScanners.

Accumulo's server-side programming model runs an \emph{iterator stack} on tablets in range of a scan, 
where each iterator is a SortedKeyValueIterator (SKVI) Java class.
An iterator stack is a set of data streams originating
at Accumulo's data sources for a specific tablet (Hadoop RFiles and cached in-memory maps), 
converging together in merge-sorts,
flowing through each iterator in the stack and at the end, sending entries to the client,
all while maintaining data emission in sorted order.

Developers add custom logic for server-side computation
by writing new SKVIs and plugging them into the iterator stack.
In return for fitting their computation in the SKVI paradigm, developers gain
distributed parallelism for free as Accumulo runs their iterators on relevant tablets simultaneously.

%The key idea behind our TableMult iterators is that instead of processing entries normally---transforming,
%reducing or expanding data flow through the stack---we leverage the iterator stack as a distributed conduit

SKVIs are reminiscent of built-in Java iterators in that they hold state 
and emit one entry at a time until finished iterating.
However, they are more powerful than Java iterators in that they can seek to a specific position
in the data stream (top-level system iterators perform actual disk seeks).

%A special caveat to iterator stacks is that Accumulo may destroy, re-create 
%and re-seek them to their last emitted key between function calls.
%Accumulo does this when it needs to switch data sources after a compaction, 
%when a client stops requesting data, or out of fairness to concurrent scans.
%Iterators have no \texttt{close} method that would grant them the lifecycle control to clean up state
%before Accumulo destroys them, and so the only safe way for an 
%iterator to use state requiring cleanup (such as opening a file or starting a thread)
%is to clean up state before returning from any call.
%Ideas discussed in \cite{ACCUMULO-3751} may lax this restriction for future Accumulo versions.

%\1 Iterators are most commonly used for streaming computation in the sense that iterators
%should run in a single pass over their source's data and not store too much state.

Iterators are most commonly used for ``reduction'' operations that transform
or eliminate entries passing through.  The Accumulo community generally discourages ``generator'' iterators 
that emit new entries not present in data sources %, not because generator iterators are impossible but
because they are easy to misuse and violate SKVI constraints by emitting entries out of order or 
relying on volatile state.
In this work, we suggest a new pattern for iterator usage as a conduit for client write operations 
that achieves the benefits of generator iterators while avoiding their constraints.

